{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d821c1",
   "metadata": {},
   "source": [
    "# Ok listen!\n",
    "93 and 346 are ACTUALLY not that bad, even I am surprised. Ignore 18..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbffe9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=10368, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from qdrant_client import QdrantClient, models\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 9 * 9, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "ds = load_dataset(\"pantelism/cats-vs-dogs\", trust_remote_code=True)\n",
    "split = ds['train'].train_test_split(test_size=0.2, seed=42)\n",
    "val_test = split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "test_dataset = val_test['test']\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "class HFDataset(Dataset):\n",
    "    def __init__(self, hf_ds, transform=None):\n",
    "        self.ds = hf_ds\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        img = item[\"image\"].convert(\"RGB\")\n",
    "        # Create a version for display before transforming it into a tensor\n",
    "        display_img = transforms.Resize((150, 150))(img)\n",
    "\n",
    "        if self.transform:\n",
    "            tensor_img = self.transform(img)\n",
    "        else:\n",
    "            tensor_img = transforms.ToTensor()(display_img)\n",
    "\n",
    "        return tensor_img, np.array(display_img)\n",
    "\n",
    "\n",
    "test_ds = HFDataset(test_dataset, transform)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "model = CNN().to(device)\n",
    "try:\n",
    "    model.load_state_dict(torch.load('cat_dog_cnn_model.pth', map_location=device))\n",
    "except FileNotFoundError:\n",
    "    exit()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e47560f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureExtractor(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super().__init__()\n",
    "        self.features = original_model.features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "feature_extractor = FeatureExtractor(model).to(device)\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64fbe4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19346/2644181824.py:6: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "collection_name = \"image_embeddings\"\n",
    "vector_size = 128 * 9 * 9\n",
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=vector_size,\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")\n",
    "\n",
    "points_to_upload = []\n",
    "with torch.no_grad():\n",
    "    for i, (batch_images, _) in enumerate(test_loader):\n",
    "        batch_images = batch_images.to(device)\n",
    "        embeddings = feature_extractor(batch_images).cpu().numpy()\n",
    "        for j, embedding in enumerate(embeddings):\n",
    "            original_index = i * test_loader.batch_size + j\n",
    "            points_to_upload.append(\n",
    "                models.PointStruct(\n",
    "                    id=str(uuid.uuid4()),\n",
    "                    vector=embedding.tolist(),\n",
    "                    payload={\"original_index\": original_index}\n",
    "                )\n",
    "            )\n",
    "\n",
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points_to_upload,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f87edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing search for image at index: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19346/2175752053.py:32: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved similarity plot: similarity_results_query_18.png\n",
      "\n",
      "Performing search for image at index: 93\n",
      "Saved similarity plot: similarity_results_query_93.png\n",
      "\n",
      "Performing search for image at index: 346\n",
      "Saved similarity plot: similarity_results_query_346.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def display_results(query_img, search_results, test_dataset, query_index):\n",
    "    fig, axes = plt.subplots(1, len(search_results) + 1, figsize=(15, 5))\n",
    "\n",
    "    axes[0].imshow(query_img)\n",
    "    axes[0].set_title(f\"Query Image ({query_index})\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    for i, result in enumerate(search_results):\n",
    "        retrieved_idx = result.payload['original_index']\n",
    "        _, retrieved_img = test_dataset[retrieved_idx]\n",
    "        axes[i+1].imshow(retrieved_img)\n",
    "        axes[i+1].set_title(f\"Idx: {retrieved_idx}\\nScore: {result.score:.3f}\")\n",
    "        axes[i+1].axis('off')\n",
    "\n",
    "    plot_filename = f\"similarity_results_query_{query_index}.png\"\n",
    "    plt.savefig(plot_filename, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved similarity plot: {plot_filename}\")\n",
    "\n",
    "k = 5 # top 5 images\n",
    "# random_indices = np.random.choice(len(test_ds), 3, replace=False)\n",
    "random_indices = [18, 93, 346] # 4 consistency\n",
    "\n",
    "for query_index in random_indices:\n",
    "    query_index_int = int(query_index)\n",
    "    print(f\"\\nPerforming search for image at index: {query_index_int}\")\n",
    "    query_tensor, query_img_display = test_ds[query_index_int]\n",
    "    query_vector = feature_extractor(query_tensor.unsqueeze(0).to(device)).detach().cpu().numpy().flatten()\n",
    "    search_results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=k\n",
    "    )\n",
    "    display_results(query_img_display, search_results, test_ds, query_index_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa500e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eng-ai-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
